# Nvidia-Triton-Inference-Server-with-Azure-ML
Using Triton to serve YOLOv models created using Azure ML Studio - AutoML for Images

This repo is a portion of a Nvidia GTC session (March 2022), developed and delivered by Rick Durham and myself, the purpose being to provide a deployment path for vision models using the power of Nvidia's Triton Inference server. This can be deployed on Azure, or on a robust x86 Edge device, depending on the need.

Please refer to the Word document for step-by-step instrutions.
